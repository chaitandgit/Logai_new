import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler


class LogAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=16, latent_dim=8):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)


class AutoEncoderDetector:
    def __init__(self, hidden_dim=16, latent_dim=8, lr=0.001,
                 batch_size=32, num_epochs=20, contamination=0.05):
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.lr = lr
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        self.contamination = contamination
        self.scaler = StandardScaler()
        self.model = None

    def fit(self, X):
        """Train autoencoder on normal log features"""
        X_scaled = self.scaler.fit_transform(X)
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

        self.model = LogAutoencoder(X.shape[1],
                                    hidden_dim=self.hidden_dim,
                                    latent_dim=self.latent_dim)
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        loss_fn = nn.MSELoss()

        for epoch in range(self.num_epochs):
            self.model.train()
            perm = torch.randperm(X_tensor.size(0))
            for i in range(0, X_tensor.size(0), self.batch_size):
                idx = perm[i:i + self.batch_size]
                batch = X_tensor[idx]

                optimizer.zero_grad()
                recon = self.model(batch)
                loss = loss_fn(recon, batch)
                loss.backward()
                optimizer.step()

    def decision_function(self, X):
        """Compute reconstruction error (anomaly score)"""
        X_scaled = self.scaler.transform(X)
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

        self.model.eval()
        with torch.no_grad():
            recon = self.model(X_tensor)
            errors = (X_tensor - recon).pow(2).numpy()
            scores = errors.mean(axis=1)  # per-row score
        return scores, errors

    def predict(self, X):
        scores, _ = self.decision_function(X)
        threshold = np.percentile(scores, 100 * (1 - self.contamination))
        return (scores > threshold).astype(int)  # 1 = anomaly, 0 = normal


from autoencoder_detector import AutoEncoderDetector

# Suppose df is your feature DataFrame (numeric only for now)
X = df[numerical_features].fillna(0).values

detector = AutoEncoderDetector(num_epochs=20, contamination=0.05)
detector.fit(X)

scores, errors = detector.decision_function(X)
labels = detector.predict(X)

df["anomaly_score"] = scores
df["anomaly_label"] = labels

# Optional: per-feature reconstruction error (to explain WHY)
error_df = pd.DataFrame(errors, columns=numerical_features)
df = pd.concat([df, error_df.add_prefix("recon_err_")], axis=1)

implement this but on csv first once satisfied can send back to es

# --- After you train + run AutoEncoderDetector ---
scores, errors = detector.decision_function(X)
labels = detector.predict(X)

# Add anomaly score & label
df["anomaly_score_autoencoder"] = scores
df["anomaly_label_autoencoder"] = labels

# Add per-feature reconstruction errors (the WHY)
error_df = pd.DataFrame(errors, columns=numerical_features)
df = pd.concat([df, error_df.add_prefix("recon_err_")], axis=1)

print("=== Sample anomaly explanations ===")
print(df.loc[df["anomaly_label_autoencoder"] == 1,
             ["anomaly_score_autoencoder"] + [c for c in df.columns if c.startswith("recon_err_")]].head())


def explain_row(row):
    top_feat = row["explanation"][0].split("=")[0].replace("recon_err_", "")
    return f"Unusual {top_feat}: value {row[top_feat]}, reconstruction error high"
    
df["explanation_text"] = df.apply(lambda r: explain_row(r), axis=1)

